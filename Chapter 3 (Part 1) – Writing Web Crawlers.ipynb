{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3880d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Python script to retrieve an arbitrary Wikipedia page\n",
    "# and produces a list of links on that page:\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for link in bs.find_all('a'):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f086fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, there are some things that you donâ€™t want to retrieve\n",
    "# Examining the links that point to article pages (as opposed to other internal pages)\n",
    "# it is possible to fine 3 aspects in common:\n",
    "\n",
    "# They reside within <div id='bodyContent'></div>;\n",
    "# The URLs does not contain colons;\n",
    "# The URLs begin with /wiki/\n",
    "\n",
    "# Revising the code to retrieve only the desired article links with the expression:\n",
    "# ^(/wiki/)((?!:).)*$\"):\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for link in bs.find('div', {'id':'bodyContent'}).find_all(\n",
    "    'a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])\n",
    "        \n",
    "# list of all article URLs that the Wikipedia article on Kevin Bacon links to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa11c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the next step, we should take this code and transform into:\n",
    "\n",
    "# A single function,getLinks (you can use another name if waht to), that takes in a Wikipedia article \n",
    "# URL of the form /wiki/ and returns a list of all linked article URLs in the same form.\n",
    "\n",
    "# A main function that calls getLinks with a starting article, chooses a random article link \n",
    "# from the returned list, and calls getLinks again, until you stop the program or until no \n",
    "# article links are found on the new page. \n",
    "\n",
    "# Generating this code:\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "# random.seed(datetime.datetime.now()) #code from the book, but not need in this part\n",
    "seed = None #code created to substitute the code from the book., because datetime will cause a warning!!!\n",
    "random.seed(seed) #code created to substitute the code from the book, because datetime will cause a warning!!!\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(articleUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    return bs.find('div', {'id':'bodyContent'}).find_all('a',\n",
    "        href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "while len(links) > 0:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
